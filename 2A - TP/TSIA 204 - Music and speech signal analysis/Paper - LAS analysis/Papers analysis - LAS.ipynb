{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJPmyAFS-yNg"
      },
      "source": [
        "# LAS - Listen, Attend and Spell Neural Network\n",
        "\n",
        "#### Arnaud Capitan\n",
        "\n",
        "## Objective:\n",
        "\n",
        "Describe formally the LAS Listen Attend and Spell model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input sequence  of filter bank spectra features, 'speech' : $\\mathbf{x} = (x_1,...,x_T)$\n",
        "\n",
        "Output sequence of characters : $\\mathbf{y} = (<\\text{start of signal}>,y_1,...,y_S,<\\text{end of signal}>)$ \n",
        "\n",
        "With $y_i \\in (a,b,c,...,z,0,...,9,<\\text{space}>,<\\text{comma}>,<\\text{periods}>,<\\text{apostrophe}>,<\\text{unknown}>)$\n",
        "\n",
        "___\n",
        "\n",
        "### Listen module : \n",
        "\n",
        "Uses a pyramidal Bidirectional Long Short-Term Memory Recurrent Neural Network, or in short pBLSTM RNN\n",
        "\n",
        "$$\\mathbf{h} = \\text{Listen}(\\mathbf{x})$$\n",
        "\n",
        "$h _i ^j = \\text{pBLSTM}(h _{i-1} ^j, [h _{2i}^{j-1}, h_{2i+1}^{j-1}])$\n",
        "\n",
        "We repeat this previous step for all the layers of our Listen module, so that in the end we reduced the input length $T$ to a feature vector $\\mathbf{h}$ of length $U$ by a factor $2^L$, $L$ being the number of layer in the Listener.\n",
        "\n",
        "___\n",
        "\n",
        "### Attend and Spell module : \n",
        "\n",
        "Uses an attention-based LSTM transducer.\n",
        "\n",
        "At every output step, generation of a probability distribution for character $y_i$ based on all the characters seen previously, using an attention mechanism :\n",
        "\n",
        "- $c_i = \\text{AttentionContext}(s_i,\\mathbf{h})$ the context based of the current decoder state $s_i$\n",
        "- $s_i = \\text{RNN}(s_{i-1},y_{i-1},c_{i-1})$ the current decoder state, based on the previous decoder state $s_{i-1}$, the previously emitted character $y_{i-1}$ and the context vector $c_{i-1}$\n",
        "- $\\mathbb{P} (y_i | \\mathbf{x}, y_{<i}) = \\text{CharacterDistribution}(s_i,c_i)$\n",
        "\n",
        "Where :\n",
        "\n",
        "- $\\text{CharacterDistribution}$ is a MLP with softmax outputs over the characters\n",
        "- $\\text{RNN}$ is a 2-layer LSTM\n",
        "- $\\text{AttentionContext}$ function is the attention-based mechanism, described as follows :\n",
        "\n",
        "$e_{i,u} = <\\phi(s_i),\\psi(h_u)>$ the scalar energy for each time step $u$\n",
        "\n",
        "$\\alpha _{i,u} = \\dfrac{\\exp (e_{i,u})}{\\sum _u \\exp (e_{i,u})}$ the probability from softmax over time steps (attention)\n",
        "\n",
        "$c_i = \\sum _u \\alpha _{i,u} h_u$ the context vector\n",
        "\n",
        "Where $\\phi$ and $\\psi$ are MLP neural networks.\n",
        "\n",
        "### Learning :\n",
        "\n",
        "The goal is to maximize the log probability with a sequence-to-sequence method conditioned on the previous characters :\n",
        "\n",
        "$$ \\max _{_\\theta} \\sum _i \\log \\mathbb{P}(y_i | \\mathbf{x},y^*_{<i},\\theta) $$\n",
        "\n",
        "10% of the time, we use the previous character distribution to avoid overfitting in the training :\n",
        "\n",
        "- $\\tilde{y}_i \\sim \\text{CharacterDistribution}(s_i,c_i)$\n",
        "- $\\max _{\\theta} \\sum _i \\log \\mathbb{P}(y_i|\\mathbf{x},\\tilde{y}_{<i},\\theta)$\n",
        "\n",
        "### Decoding :\n",
        "\n",
        "Find the most likely character sequence given the input acoustics :\n",
        "\n",
        "$$ \\hat{y} = \\argmax _y \\log \\mathbb{P}(\\mathbf{y}|\\mathbf{x})$$\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
